{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Basics: A Comprehensive Guide\n",
    "\n",
    "This notebook demonstrates fundamental prompt engineering techniques using current AI models (GPT-5, Claude 4.1, XAI Grok) with practical examples and validation methods.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand core prompting principles\n",
    "2. Learn to craft effective prompts for different use cases\n",
    "3. Master prompt validation and optimization techniques\n",
    "4. Apply prompting to real-world scenarios\n",
    "5. Implement best practices for AI-assisted content creation\n",
    "\n",
    "## Prerequisites\n",
    "- Python 3.8+\n",
    "- API keys for OpenAI, Anthropic, and XAI\n",
    "- Basic understanding of LLM capabilities\n",
    "- Familiarity with JSON and API calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, let's install the required libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai anthropic requests python-dotenv\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set up API keys\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "XAI_API_KEY = os.getenv('XAI_API_KEY')\n",
    "\n",
    "print(\"‚úÖ Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Core Prompting Techniques\n",
    "\n",
    "Let's explore the fundamental prompting approaches with current 2025 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptEngineer:\n",
    "    \"\"\"A comprehensive prompt engineering class for 2025 AI models\"\"\"\n",
    "    \n",
    "    def __init__(self, model_preference: str = 'gpt-5'):\n",
    "        self.model_preference = model_preference\n",
    "        self.conversation_history = []\n",
    "        \n",
    "    def zero_shot_prompt(self, task: str, context: str = \"\") -> str:\n",
    "        \"\"\"Create a zero-shot prompt for direct task completion\"\"\"\n",
    "        prompt = f\"\"\"{context}\n",
    "        \n",
    "Task: {task}\n",
    "\n",
    "Instructions: Complete the task directly without additional examples.\n",
    "Provide a clear, accurate, and well-structured response.\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def few_shot_prompt(self, task: str, examples: List[Dict], context: str = \"\") -> str:\n",
    "        \"\"\"Create a few-shot prompt with examples\"\"\"\n",
    "        prompt_parts = [context] if context else []\n",
    "        prompt_parts.append(\"Examples:\")\n",
    "        \n",
    "        for i, example in enumerate(examples, 1):\n",
    "            prompt_parts.append(f\"Example {i}:\")\n",
    "            prompt_parts.append(f\"Input: {example['input']}\")\n",
    "            prompt_parts.append(f\"Output: {example['output']}\")\n",
    "            prompt_parts.append(\"\")\n",
    "        \n",
    "        prompt_parts.append(f\"Task: {task}\")\n",
    "        return \"\\n\".join(prompt_parts)\n",
    "    \n",
    "    def chain_of_thought_prompt(self, problem: str, context: str = \"\") -> str:\n",
    "        \"\"\"Create a Chain-of-Thought prompting approach\"\"\"\n",
    "        prompt = f\"\"\"{context}\n",
    "        \n",
    "Problem: {problem}\n",
    "\n",
    "Instructions: Think step-by-step to solve this problem. Break it down into logical steps:\n",
    "1. Understand the core problem\n",
    "2. Identify key variables and constraints\n",
    "3. Consider different approaches\n",
    "4. Work through the solution systematically\n",
    "5. Verify the solution\n",
    "\n",
    "Show your reasoning at each step and provide a final answer.\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def role_based_prompt(self, role: str, task: str, context: str = \"\") -> str:\n",
    "        \"\"\"Create a role-based prompt for specialized expertise\"\"\"\n",
    "        prompt = f\"\"\"You are a {role}. Use your expertise to complete the following task:\n",
    "\n",
    "{context}\n",
    "\n",
    "Task: {task}\n",
    "\n",
    "Apply your specialized knowledge and provide a professional response.\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def validation_prompt(self, content: str, criteria: List[str]) -> str:\n",
    "        \"\"\"Create a validation prompt to check content quality\"\"\"\n",
    "        criteria_text = \"\\n\".join(f\"- {criterion}\" for criterion in criteria)\n",
    "        \n",
    "        prompt = f\"\"\"Please validate the following content against these criteria:\n",
    "\n",
    "Content to validate:\n",
    "{content}\n",
    "\n",
    "Validation Criteria:\n",
    "{criteria_text}\n",
    "\n",
    "Provide a detailed validation report including:\n",
    "1. Compliance with each criterion (Met/Partially Met/Not Met)\n",
    "2. Specific issues or concerns\n",
    "3. Suggestions for improvement\n",
    "4. Overall quality score (1-10)\n",
    "5. Recommendations for revision\"\"\"\n",
    "        return prompt\n",
    "\n",
    "# Initialize the prompt engineer\n",
    "engineer = PromptEngineer()\n",
    "print(\"‚úÖ PromptEngineer initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Integration Examples\n",
    "\n",
    "Let's create functions to interact with different 2025 AI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import anthropic\n",
    "import requests\n",
    "\n",
    "# Set up API clients\n",
    "if OPENAI_API_KEY:\n",
    "    openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "    \n",
    "if ANTHROPIC_API_KEY:\n",
    "    anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "def query_gpt5(prompt: str, temperature: float = 0.7) -> str:\n",
    "    \"\"\"Query GPT-5 with optimized prompting\"\"\"\n",
    "    if not OPENAI_API_KEY:\n",
    "        return \"OpenAI API key not configured\"\n",
    "    \n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-5\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=4000\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error querying GPT-5: {str(e)}\"\n",
    "\n",
    "def query_claude41(prompt: str, temperature: float = 0.7) -> str:\n",
    "    \"\"\"Query Claude 4.1 with advanced prompting\"\"\"\n",
    "    if not ANTHROPIC_API_KEY:\n",
    "        return \"Anthropic API key not configured\"\n",
    "    \n",
    "    try:\n",
    "        message = anthropic_client.messages.create(\n",
    "            model=\"claude-4-1-opus-20241221\",\n",
    "            max_tokens=4000,\n",
    "            temperature=temperature,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return message.content[0].text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error querying Claude 4.1: {str(e)}\"\n",
    "\n",
    "def query_grok(prompt: str, temperature: float = 0.7) -> str:\n",
    "    \"\"\"Query XAI Grok with real-time capabilities\"\"\"\n",
    "    if not XAI_API_KEY:\n",
    "        return \"XAI API key not configured\"\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {XAI_API_KEY}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            'model': 'grok-4',\n",
    "            'messages': [{'role': 'user', 'content': prompt}],\n",
    "            'temperature': temperature,\n",
    "            'max_tokens': 4000\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            'https://api.x.ai/v1/chat/completions',\n",
    "            headers=headers,\n",
    "            json=data\n",
    "        )\n",
    "        \n",
    "        return response.json()['choices'][0]['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error querying Grok: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Model integration functions created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Practical Prompting Examples\n",
    "\n",
    "Let's explore different prompting techniques with real examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Zero-shot prompting for content creation\n",
    "content_task = \"Write a 500-word article about the impact of AI on academic research in 2025.\"\n",
    "context = \"Focus on positive developments, ethical considerations, and future implications.\"\n",
    "\n",
    "zero_shot_prompt = engineer.zero_shot_prompt(content_task, context)\n",
    "print(\"Zero-shot Prompt:\")\n",
    "print(zero_shot_prompt[:300] + \"...\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Example 2: Few-shot prompting for structured output\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"The study found that exercise improves memory retention in older adults.\",\n",
    "        \"output\": \"{\\\"topic\\\": \\\"memory\\\", \\\"intervention\\\": \\\"exercise\\\", \\\"population\\\": \\\"older adults\\\", \\\"finding\\\": \\\"positive\\\", \\\"confidence\\\": \\\"high\\\"}\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Meditation reduced stress levels among college students in the experiment.\",\n",
    "        \"output\": \"{\\\"topic\\\": \\\"stress\\\", \\\"intervention\\\": \\\"meditation\\\", \\\"population\\\": \\\"college students\\\", \\\"finding\\\": \\\"positive\\\", \\\"confidence\\\": \\\"high\\\"}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "structured_task = \"AI-powered tutoring systems enhanced learning outcomes for elementary students.\"\n",
    "few_shot_prompt = engineer.few_shot_prompt(structured_task, examples)\n",
    "print(\"Few-shot Prompt:\")\n",
    "print(few_shot_prompt[:400] + \"...\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation and Quality Assessment\n",
    "\n",
    "Let's create validation functions to assess AI-generated content quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Tuple, List\n",
    "\n",
    "class ContentValidator:\n",
    "    \"\"\"Comprehensive content validation for AI-generated materials\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fact_check_keywords = [\n",
    "            'according to', 'research shows', 'study found',\n",
    "            'data indicates', 'evidence suggests', 'statistics show'\n",
    "        ]\n",
    "    \n",
    "    def validate_academic_content(self, content: str) -> Dict:\n",
    "        \"\"\"Validate academic content for quality and accuracy\"\"\"\n",
    "        validation_results = {\n",
    "            'length_check': self._check_length(content),\n",
    "            'citation_check': self._check_citations(content),\n",
    "            'structure_check': self._check_structure(content),\n",
    "            'originality_check': self._check_originality(content),\n",
    "            'clarity_check': self._check_clarity(content),\n",
    "            'fact_claims': self._identify_fact_claims(content)\n",
    "        }\n",
    "        \n",
    "        # Calculate overall score\n",
    "        scores = [result['score'] for result in validation_results.values()]\n",
    "        validation_results['overall_score'] = sum(scores) / len(scores)\n",
    "        \n",
    "        return validation_results\n",
    "    \n",
    "    def _check_length(self, content: str) -> Dict:\n",
    "        \"\"\"Check content length appropriateness\"\"\"\n",
    "        word_count = len(content.split())\n",
    "        \n",
    "        if word_count < 100:\n",
    "            return {'score': 3, 'status': 'Too short', 'word_count': word_count}\n",
    "        elif word_count < 500:\n",
    "            return {'score': 7, 'status': 'Appropriate', 'word_count': word_count}\n",
    "        elif word_count < 1500:\n",
    "            return {'score': 8, 'status': 'Good length', 'word_count': word_count}\n",
    "        else:\n",
    "            return {'score': 6, 'status': 'Very long', 'word_count': word_count}\n",
    "    \n",
    "    def _check_citations(self, content: str) -> Dict:\n",
    "        \"\"\"Check for proper citations and references\"\"\"\n",
    "        citation_patterns = [\n",
    "            r'\\([^)]*\\d{4}[^)]*\\)',  # (Author, Year)\n",
    "            r'\\b\\d{4}\\b',  # Year references\n",
    "            r'et al\\.',  # et al. citations\n",
    "            r'According to',  # Attribution phrases\n",
    "        ]\n",
    "        \n",
    "        citation_count = sum(len(re.findall(pattern, content)) for pattern in citation_patterns)\n",
    "        \n",
    "        if citation_count == 0:\n",
    "            return {'score': 2, 'status': 'No citations found', 'count': citation_count}\n",
    "        elif citation_count < 3:\n",
    "            return {'score': 5, 'status': 'Few citations', 'count': citation_count}\n",
    "        elif citation_count < 7:\n",
    "            return {'score': 8, 'status': 'Well cited', 'count': citation_count}\n",
    "        else:\n",
    "            return {'score': 9, 'status': 'Extensively cited', 'count': citation_count}\n",
    "    \n",
    "    def _check_structure(self, content: str) -> Dict:\n",
    "        \"\"\"Check content structure and organization\"\"\"\n",
    "        structure_indicators = [\n",
    "            'introduction', 'background', 'methodology', 'results',\n",
    "            'discussion', 'conclusion', 'references', 'abstract'\n",
    "        ]\n",
    "        \n",
    "        found_indicators = [indicator for indicator in structure_indicators \n",
    "                          if indicator.lower() in content.lower()]\n",
    "        \n",
    "        structure_score = len(found_indicators) / len(structure_indicators) * 10\n",
    "        \n",
    "        return {\n",
    "            'score': min(10, structure_score + 3),  # Base score of 3\n",
    "            'status': f\"{len(found_indicators)}/{len(structure_indicators)} sections found\",\n",
    "            'found_sections': found_indicators\n",
    "        }\n",
    "    \n",
    "    def _check_originality(self, content: str) -> Dict:\n",
    "        \"\"\"Assess content originality and unique insights\"\"\"\n",
    "        # Check for generic phrases and overused terms\n",
    "        generic_phrases = [\n",
    "            'in conclusion', 'it is important to note', 'research shows',\n",
    "            'many people believe', 'it is widely known'\n",
    "        ]\n",
    "        \n",
    "        generic_count = sum(content.lower().count(phrase) for phrase in generic_phrases)\n",
    "        \n",
    "        # Check for specific examples and details\n",
    "        specific_indicators = [\n",
    "            r'\\d+%',  # Specific percentages\n",
    "            r'\\d+ studies',  # Specific study counts\n",
    "            r'\\b\\d{4}\\b',  # Specific years\n",
    "            'example', 'case study', 'specifically'\n",
    "        ]\n",
    "        \n",
    "        specific_count = sum(len(re.findall(indicator, content, re.IGNORECASE)) \n",
    "                           for indicator in specific_indicators)\n",
    "        \n",
    "        originality_score = max(1, min(10, 5 + specific_count - generic_count))\n",
    "        \n",
    "        return {\n",
    "            'score': originality_score,\n",
    "            'status': 'Good specificity' if specific_count > generic_count else 'More specific examples needed',\n",
    "            'specific_indicators': specific_count,\n",
    "            'generic_phrases': generic_count\n",
    "        }\n",
    "    \n",
    "    def _check_clarity(self, content: str) -> Dict:\n",
    "        \"\"\"Assess content clarity and readability\"\"\"\n",
    "        sentences = re.split(r'[.!?]+', content)\n",
    "        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)\n",
    "        \n",
    "        # Check for complex jargon\n",
    "        complex_words = ['methodology', 'paradigm', 'epistemology', 'ontology', 'phenomenology']\n",
    "        complex_count = sum(content.lower().count(word) for word in complex_words)\n",
    "        \n",
    "        clarity_score = 10\n",
    "        issues = []\n",
    "        \n",
    "        if avg_sentence_length > 25:\n",
    "            clarity_score -= 2\n",
    "            issues.append('Long sentences detected')\n",
    "        \n",
    "        if complex_count > 5:\n",
    "            clarity_score -= 1\n",
    "            issues.append('High technical jargon')\n",
    "        \n",
    "        return {\n",
    "            'score': max(1, clarity_score),\n",
    "            'status': 'Clear and readable' if clarity_score >= 8 else 'Can be improved',\n",
    "            'avg_sentence_length': avg_sentence_length,\n",
    "            'issues': issues\n",
    "        }\n",
    "    \n",
    "    def _identify_fact_claims(self, content: str) -> List[Dict]:\n",
    "        \"\"\"Identify factual claims that need verification\"\"\"\n",
    "        fact_claims = []\n",
    "        \n",
    "        # Look for fact-indicating phrases\n",
    "        for keyword in self.fact_check_keywords:\n",
    "            positions = [m.start() for m in re.finditer(keyword, content, re.IGNORECASE)]\n",
    "            for pos in positions:\n",
    "                # Extract surrounding context\n",
    "                start = max(0, pos - 50)\n",
    "                end = min(len(content), pos + 100)\n",
    "                context = content[start:end].strip()\n",
    "                \n",
    "                fact_claims.append({\n",
    "                    'keyword': keyword,\n",
    "                    'context': context,\n",
    "                    'position': pos,\n",
    "                    'needs_verification': True\n",
    "                })\n",
    "        \n",
    "        return fact_claims\n",
    "\n",
    "# Initialize validator\n",
    "validator = ContentValidator()\n",
    "print(\"‚úÖ ContentValidator initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Validation Examples\n",
    "\n",
    "Let's test the validation system with sample content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample academic content for validation\n",
    "sample_content = \"\"\"\n",
    "The Impact of Artificial Intelligence on Academic Research in 2025\n",
    "\n",
    "Introduction\n",
    "Artificial intelligence has revolutionized academic research across multiple disciplines. This paper examines the transformative effects of AI tools on research methodologies, data analysis, and scholarly communication.\n",
    "\n",
    "Background\n",
    "Recent developments in large language models, particularly GPT-5 and Claude 4.1, have enabled new forms of research assistance. According to Smith et al. (2024), these tools can increase research productivity by up to 40%.\n",
    "\n",
    "Methodology\n",
    "This study analyzed 500 research papers published in 2025 to assess AI tool usage patterns. Data was collected from major academic databases including PubMed, IEEE Xplore, and Google Scholar.\n",
    "\n",
    "Results\n",
    "The analysis revealed that 78% of papers in computer science and 45% in social sciences utilized AI tools for literature review and data analysis. Specifically, GPT-5 was used in 34% of cases, while Claude 4.1 appeared in 28% of the analyzed papers.\n",
    "\n",
    "Discussion\n",
    "These findings suggest that AI tools are becoming integral to modern research workflows. However, concerns remain about academic integrity and the need for proper citation practices.\n",
    "\n",
    "Conclusion\n",
    "As AI continues to evolve, researchers must develop appropriate guidelines for ethical AI usage in academic work. Future research should focus on developing standards for AI tool citation and validation.\n",
    "\n",
    "References\n",
    "Smith, J., et al. (2024). AI in Academic Research: A Comprehensive Review. Journal of Higher Education Technology, 15(2), 45-67.\n",
    "Johnson, A., & Williams, B. (2025). Large Language Models in Scholarly Communication. Research Today, 8(1), 12-25.\n",
    "\"\"\"\n",
    "\n",
    "# Validate the sample content\n",
    "validation_results = validator.validate_academic_content(sample_content)\n",
    "\n",
    "print(\"üìä Content Validation Results:\")\n",
    "print(f\"Overall Score: {validation_results['overall_score']:.1f}/10\")\n",
    "print(\"\\nDetailed Results:\")\n",
    "for check, result in validation_results.items():\n",
    "    if check != 'overall_score':\n",
    "        print(f\"{check}: Score {result['score']}/10 - {result['status']}\")\n",
    "\n",
    "print(f\"\\nüìã Fact Claims Found: {len(validation_results['fact_claims'])}\")\n",
    "for i, claim in enumerate(validation_results['fact_claims'][:3], 1):\n",
    "    print(f\"{i}. {claim['keyword']}: {claim['context'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Prompting Techniques\n",
    "\n",
    "Let's explore advanced prompting techniques with current 2025 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced prompting with model comparison\n",
    "def compare_models_on_task(task: str, models: List[str] = None) -> Dict:\n",
    "    \"\"\"Compare different models on the same task\"\"\"\n",
    "    if models is None:\n",
    "        models = ['gpt-5', 'claude-4.1', 'grok-4']\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"ü§ñ Querying {model.upper()}...\")\n",
    "        \n",
    "        # Create enhanced prompt for 2025 models\n",
    "        enhanced_prompt = f\"\"\"You are using {model.upper()}, one of the most advanced AI systems available in 2025.\n",
    "        \n",
    "Task: {task}\n",
    "\n",
    "Instructions:\n",
    "1. Use your extensive knowledge and real-time access (where available)\n",
    "2. Provide detailed, accurate, and well-structured responses\n",
    "3. Include specific examples and current data\n",
    "4. Consider ethical implications and limitations\n",
    "5. Use your unique capabilities to enhance the response\n",
    "\n",
    "Please provide your response in a clear, professional format.\"\"\"\n",
    "        \n",
    "        if model.lower() == 'gpt-5':\n",
    "            response = query_gpt5(enhanced_prompt)\n",
    "        elif model.lower() == 'claude-4.1':\n",
    "            response = query_claude41(enhanced_prompt)\n",
    "        elif model.lower() == 'grok-4':\n",
    "            response = query_grok(enhanced_prompt)\n",
    "        else:\n",
    "            response = f\"Model {model} not implemented\"\n",
    "        \n",
    "        results[model] = {\n",
    "            'response': response[:500] + \"...\" if len(response) > 500 else response,\n",
    "            'length': len(response),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example task for model comparison\n",
    "research_task = \"Analyze the current state of AI ethics in academic research as of 2025, including recent developments, challenges, and future directions.\"\n",
    "\n",
    "print(\"üîç Comparing models on AI ethics analysis...\")\n",
    "try:\n",
    "    comparison_results = compare_models_on_task(research_task, ['gpt-5'])\n",
    "    \n",
    "    for model, result in comparison_results.items():\n",
    "        print(f\"\\nüìù {model.upper()} Response:\")\n",
    "        print(result['response'])\n",
    "        print(f\"üìè Length: {result['length']} characters\")\n",
    "        print(f\"‚è∞ Generated: {result['timestamp']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in model comparison: {e}\")\n",
    "    print(\"Note: API keys may not be configured for all models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices and Guidelines\n",
    "\n",
    "Let's create a comprehensive prompt optimization framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptOptimizer:\n",
    "    \"\"\"Advanced prompt optimization for 2025 AI models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.optimization_techniques = {\n",
    "            'context_engineering': self._optimize_context,\n",
    "            'role_assignment': self._optimize_role,\n",
    "            'output_formatting': self._optimize_format,\n",
    "            'constraint_specification': self._optimize_constraints,\n",
    "            'validation_integration': self._optimize_validation\n",
    "        }\n",
    "    \n",
    "    def optimize_prompt(self, base_prompt: str, techniques: List[str] = None) -> str:\n",
    "        \"\"\"Apply multiple optimization techniques to a base prompt\"\"\"\n",
    "        if techniques is None:\n",
    "            techniques = list(self.optimization_techniques.keys())\n",
    "        \n",
    "        optimized_prompt = base_prompt\n",
    "        \n",
    "        for technique in techniques:\n",
    "            if technique in self.optimization_techniques:\n",
    "                optimized_prompt = self.optimization_techniques[technique](optimized_prompt)\n",
    "        \n",
    "        return optimized_prompt\n",
    "    \n",
    "    def _optimize_context(self, prompt: str) -> str:\n",
    "        \"\"\"Add context optimization elements\"\"\"\n",
    "        context_addition = \"\"\"\n",
    "Context Optimization:\n",
    "- Consider the broader implications and context\n",
    "- Include relevant background information\n",
    "- Account for current developments and trends\n",
    "- Consider multiple perspectives and viewpoints\n",
    "\"\"\"\n",
    "        return prompt + context_addition\n",
    "    \n",
    "    def _optimize_role(self, prompt: str) -> str:\n",
    "        \"\"\"Add role optimization elements\"\"\"\n",
    "        role_addition = \"\"\"\n",
    "Role Specification:\n",
    "- You are an expert in this domain\n",
    "- Apply your specialized knowledge and experience\n",
    "- Consider professional standards and best practices\n",
    "- Provide insights based on current research and developments\n",
    "\"\"\"\n",
    "        return prompt + role_addition\n",
    "    \n",
    "    def _optimize_format(self, prompt: str) -> str:\n",
    "        \"\"\"Add output formatting optimization\"\"\"\n",
    "        format_addition = \"\"\"\n",
    "Output Requirements:\n",
    "- Use clear, structured formatting\n",
    "- Include specific examples and evidence\n",
    "- Provide actionable recommendations\n",
    "- Use professional and academic language\n",
    "- Include relevant citations and references\n",
    "\"\"\"\n",
    "        return prompt + format_addition\n",
    "    \n",
    "    def _optimize_constraints(self, prompt: str) -> str:\n",
    "        \"\"\"Add constraint optimization elements\"\"\"\n",
    "        constraint_addition = \"\"\"\n",
    "Constraints and Guidelines:\n",
    "- Ensure accuracy and factual correctness\n",
    "- Consider ethical implications\n",
    "- Maintain academic integrity standards\n",
    "- Follow proper citation practices\n",
    "- Acknowledge limitations and uncertainties\n",
    "\"\"\"\n",
    "        return prompt + constraint_addition\n",
    "    \n",
    "    def _optimize_validation(self, prompt: str) -> str:\n",
    "        \"\"\"Add validation optimization elements\"\"\"\n",
    "        validation_addition = \"\"\"\n",
    "Validation Instructions:\n",
    "- Cross-reference information with reliable sources\n",
    "- Include confidence levels for claims\n",
    "- Note any assumptions or limitations\n",
    "- Suggest methods for verification\n",
    "- Provide uncertainty quantification where appropriate\n",
    "\"\"\"\n",
    "        return prompt + validation_addition\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = PromptOptimizer()\n",
    "\n",
    "# Example optimization\n",
    "base_prompt = \"Explain the impact of AI on academic research.\"\n",
    "optimized_prompt = optimizer.optimize_prompt(base_prompt)\n",
    "\n",
    "print(\"üîß Base Prompt:\")\n",
    "print(base_prompt)\n",
    "print(\"\\nüöÄ Optimized Prompt:\")\n",
    "print(optimized_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. JSON Schema Validation Examples\n",
    "\n",
    "Let's create practical JSON schema validation for AI outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from jsonschema import validate, ValidationError\n",
    "\n",
    "# Define JSON schemas for different AI output types\n",
    "RESEARCH_ANALYSIS_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"title\": {\"type\": \"string\", \"minLength\": 10, \"maxLength\": 200},\n",
    "        \"authors\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "        \"publication_year\": {\"type\": \"integer\", \"minimum\": 1900, \"maximum\": 2025},\n",
    "        \"methodology\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"type\": {\"type\": \"string\"},\n",
    "                \"sample_size\": {\"type\": \"integer\"},\n",
    "                \"data_collection_period\": {\"type\": \"string\"}\n",
    "            },\n",
    "            \"required\": [\"type\", \"sample_size\"]\n",
    "        },\n",
    "        \"key_findings\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "        \"limitations\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "        \"doi\": {\"type\": \"string\"}\n",
    "    },\n",
    "    \"required\": [\"title\", \"authors\", \"publication_year\", \"key_findings\"]\n",
    "}\n",
    "\n",
    "MARKET_ANALYSIS_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"market_name\": {\"type\": \"string\"},\n",
    "        \"market_size\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"value\": {\"type\": \"number\"},\n",
    "                \"currency\": {\"type\": \"string\"},\n",
    "                \"year\": {\"type\": \"integer\"}\n",
    "            }\n",
    "        },\n",
    "        \"growth_rate\": {\"type\": \"number\", \"minimum\": -100, \"maximum\": 1000},\n",
    "        \"key_drivers\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "        \"major_competitors\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "        \"market_trends\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "        \"opportunities\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "        \"threats\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "        \"confidence_score\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1}\n",
    "    },\n",
    "    \"required\": [\"market_name\", \"market_size\", \"key_drivers\"]\n",
    "}\n",
    "\n",
    "def validate_ai_output(output: str, schema: dict) -> Dict:\n",
    "    \"\"\"Validate AI output against JSON schema\"\"\"\n",
    "    try:\n",
    "        # Try to parse as JSON\n",
    "        parsed_output = json.loads(output)\n",
    "        \n",
    "        # Validate against schema\n",
    "        validate(instance=parsed_output, schema=schema)\n",
    "        \n",
    "        return {\n",
    "            'valid': True,\n",
    "            'parsed_data': parsed_output,\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\n",
    "            'valid': False,\n",
    "            'parsed_data': None,\n",
    "            'errors': [f\"JSON parsing error: {str(e)}\"]\n",
    "        }\n",
    "        \n",
    "    except ValidationError as e:\n",
    "        return {\n",
    "            'valid': False,\n",
    "            'parsed_data': None,\n",
    "            'errors': [f\"Schema validation error: {str(e)}\"]\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'valid': False,\n",
    "            'parsed_data': None,\n",
    "            'errors': [f\"Unexpected error: {str(e)}\"]\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "sample_research_json = '''\n",
    "{\n",
    "  \"title\": \"The Impact of AI on Academic Research Productivity\",\n",
    "  \"authors\": [\"Dr. Sarah Johnson\", \"Prof. Michael Chen\"],\n",
    "  \"publication_year\": 2025,\n",
    "  \"methodology\": {\n",
    "    \"type\": \"mixed methods\",\n",
    "    \"sample_size\": 500,\n",
    "    \"data_collection_period\": \"2024-2025\"\n",
    "  },\n",
    "  \"key_findings\": [\n",
    "    \"AI tools increased research productivity by 35%\",\n",
    "    \"Most significant impact in literature review phase\",\n",
    "    \"Concerns about academic integrity remain\"\n",
    "  ],\n",
    "  \"limitations\": [\n",
    "    \"Sample limited to STEM disciplines\",\n",
    "    \"Self-reported productivity measures\"\n",
    "  ],\n",
    "  \"doi\": \"10.1234/research.2025.001\"\n",
    "}\n",
    "'''\n",
    "\n",
    "# Validate the sample JSON\n",
    "validation_result = validate_ai_output(sample_research_json, RESEARCH_ANALYSIS_SCHEMA)\n",
    "\n",
    "print(\"üìã JSON Schema Validation Results:\")\n",
    "print(f\"Valid: {validation_result['valid']}\")\n",
    "\n",
    "if validation_result['valid']:\n",
    "    print(\"‚úÖ JSON is valid according to schema\")\n",
    "    print(f\"üìä Parsed data keys: {list(validation_result['parsed_data'].keys())}\")\n",
    "else:\n",
    "    print(\"‚ùå JSON validation failed:\")\n",
    "    for error in validation_result['errors']:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "print(\"\\nüîç Validation completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "This notebook has demonstrated key concepts in prompt engineering for 2025 AI models:\n",
    "\n",
    "### Key Learnings\n",
    "1. **Model Capabilities**: Understanding GPT-5, Claude 4.1, and XAI Grok capabilities\n",
    "2. **Prompt Techniques**: Zero-shot, few-shot, and Chain-of-Thought prompting\n",
    "3. **Validation Methods**: Content validation and JSON schema compliance\n",
    "4. **Best Practices**: Ethical AI usage and academic integrity\n",
    "5. **Practical Implementation**: Working code examples and API integrations\n",
    "\n",
    "### Next Steps\n",
    "1. **Configure API Keys**: Set up actual API keys for testing\n",
    "2. **Explore Advanced Features**: Try multimodal prompting and agentic workflows\n",
    "3. **Customize Schemas**: Adapt JSON schemas for your specific use cases\n",
    "4. **Build Applications**: Create practical applications using these techniques\n",
    "5. **Contribute Back**: Share improvements and new techniques with the community\n",
    "\n",
    "### Resources\n",
    "- [Prompting-Gold-Standard Repository](https://github.com/llMr-Sweetll/Prompting-Gold-Standard)\n",
    "- [OpenAI GPT-5 Documentation](https://platform.openai.com/docs)\n",
    "- [Anthropic Claude 4.1 Guide](https://docs.anthropic.com/claude)\n",
    "- [XAI Grok API Documentation](https://x.ai/grok)\n",
    "\n",
    "Remember: The field of AI prompting is rapidly evolving. Stay current with latest developments and always prioritize ethical usage and validation of AI-generated content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
